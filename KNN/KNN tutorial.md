## 1. Algorithm
### 1.1 Intuition
- The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. 
- Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.
### 1.2 Mathematics
### 1.3 Choosing k
### 1.4 Distance measures

## 2. Decision boundary
- Voronoi tesselation
  - partitions space into regions
  - boundary: points at the same distance from two different training examples
<p align="center">
<img src=https://www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1-separated-2.png width="300" height="300" alt="SVM" align=center>
## 3. Examples/Applications

## 4. Extensions
### 4.1 Parzen Windows and Kernels
### 4.2 K-D trees
### 4.3 Inverted list examples
### 4.4 Locality-Sensitive hashing

## 5. Practical issues

## 6. Pros and cons