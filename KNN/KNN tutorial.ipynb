{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Algorithm\n",
    "### 1.1 Intuition\n",
    "- The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. \n",
    "- Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "### 1.2 Mathematics\n",
    "- Step 1: $D(x,x_i)$ to every training example $x_i$;\n",
    "- Step 2: select $k$ closest instances $x_{i1},...,x_{ik}$ and their labels $y_{i1},...,y_{ik}$;\n",
    "- Step 3: output the class y which is the mode of $y_{i1},...,y_{ik}$.\n",
    "\n",
    "### 1.3 Choosing k\n",
    "- k has strong effect on kNN performance:\n",
    "  - large value: everything classified as the most probable class;\n",
    "  - small value: highly variable, unstable deicision boundary.\n",
    "- Set the validation set, draw the learning rate (x-->k, y-->validation error), pick k that gives a ```reasonably good``` generalization performance.\n",
    "### 1.4 Distance measures\n",
    "- The measure of distances has strong effect on performance as well:\n",
    "  - Minkowski distance (p-norm):\n",
    "  $$ D(x,x') = \\sqrt[p]{\\sum_d \\left|x_d - x'_d\\right|^p} $$\n",
    "    - 1) $p = 2$: Euclidean\n",
    "    - 2) $p = 1$: Manhattan\n",
    "    <p align=\"center\">\n",
    "    <img src=images/knn2.jpg width=\"300\" height=\"300\" alt=\"knn2\" align=center>\n",
    "    - 3) $p -> \\infty$: $\\underset{d}{\\text{max}}\\left|x_d-x'_d\\right|$\n",
    "    - 4) $p -> 0$: number of non-zero differences\n",
    "    $$ D(x,x') = \\sum_d 1_{x_d\\not=x'_d} $$\n",
    "  - Custom distance measures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Decision boundary\n",
    "- Voronoi tesselation\n",
    "  - partitions space into regions\n",
    "  - boundary: points at the same distance from two different training examples\n",
    "<p align=\"center\">\n",
    "<img src=images/KNN1.jpg width=\"300\" height=\"300\" alt=\"knn\" align=center>\n",
    "\n",
    "## 3. Examples/Applications\n",
    "\n",
    "## 4. Extensions\n",
    "### 4.1 Parzen Windows and Kernels\n",
    "### 4.2 K-D trees\n",
    "### 4.3 Inverted list examples\n",
    "### 4.4 Locality-Sensitive hashing\n",
    "\n",
    "## 5. Practical issues\n",
    "\n",
    "## 6. Pros and cons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}