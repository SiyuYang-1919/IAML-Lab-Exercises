{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Algorithm\n",
    "### 1.1 Intuition\n",
    "- The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. \n",
    "- Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "### 1.2 Mathematics\n",
    "- kNN classification:\n",
    "  - Step 1: $D(x,x_i)$ to every training example $x_i$;\n",
    "  - Step 2: select $k$ closest instances $x_{i1},...,x_{ik}$ and their labels $y_{i1},...,y_{ik}$;\n",
    "  - Step 3: output the class y which is the mode of $y_{i1},...,y_{ik}$.\n",
    "- kNN regression:\n",
    "  - Step 1: $D(x,x_i)$ to every training example $x_i$;\n",
    "  - Step 2: select $k$ closest instances $x_{i1},...,x_{ik}$ and their labels $y_{i1},...,y_{ik}$;\n",
    "  - Step 3: output the mean of $y_{i1},...,y_{ik}$\n",
    "  $$ \\hat{y}=\\frac{1}{k}\\sum_{j=1}^k y_{ij} $$\n",
    "\n",
    "### 1.3 Choosing k\n",
    "- k has strong effect on kNN performance:\n",
    "  - large value: everything classified as the most probable class;\n",
    "  - small value: highly variable, unstable deicision boundary.\n",
    "- Set the validation set, draw the learning rate (x-->k, y-->validation error), pick k that gives a ```reasonably good``` generalization performance.\n",
    "### 1.4 Distance measures\n",
    "- The measure of distances has strong effect on performance as well:\n",
    "  - Minkowski distance (p-norm):\n",
    "  $$ D(x,x') = \\sqrt[p]{\\sum_d \\left|x_d - x'_d\\right|^p} $$\n",
    "    - 1) $p = 2$: Euclidean\n",
    "    - 2) $p = 1$: Manhattan\n",
    "    <p align=\"center\">\n",
    "    <img src=images/knn2.jpg width=\"300\" height=\"300\" alt=\"knn2\" align=center>\n",
    "    - 3) $p -> \\infty$: $\\underset{d}{\\text{max}}\\left|x_d-x'_d\\right|$\n",
    "    - 4) $p -> 0$: number of non-zero differences\n",
    "    $$ D(x,x') = \\sum_d 1_{x_d\\not=x'_d} $$\n",
    "  - Custom distance measures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Decision boundary\n",
    "- Voronoi tesselation\n",
    "  - partitions space into regions\n",
    "  - boundary: points at the same distance from two different training examples\n",
    "<p align=\"center\">\n",
    "<img src=images/KNN1.jpg width=\"300\" height=\"300\" alt=\"knn\" align=center>\n",
    "\n",
    "## 3. Examples/Applications\n",
    "- Handwritten digits\n",
    "## 4. Extensions\n",
    "### 4.1 Parzen Windows and Kernels\n",
    "Instead of defining $k$, Parzen Windows define the radius of an area, within which the training examples are used to predict new instances.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/knn3.jpg width=\"300\" height=\"300\" alt=\"knn\" align=center>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/knn4.jpg width=\"300\" height=\"300\" alt=\"knn\" align=center>\n",
    "\n",
    "the hypothesis function is:\n",
    "$$ h(x) = \\text{sgn}[\\sum_{i:x_i\\in{R(x)}}y_i] $$\n",
    "that is,\n",
    "$$ h(x) = \\text{sgn}[\\sum_{i}y_iÂ·1_{||x_i-x||\\leq R}] $$\n",
    "kernalize the equation above,\n",
    "$$ h(x) = [\\sum_{i}y_iK(x_i,x)] $$\n",
    "<p align=\"center\">\n",
    "<img src=images/knn5.jpg width=\"500\" height=\"200\" alt=\"knn\" align=center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.2 Making kNN fast\n",
    "- Reduce d: simple feature selection;\n",
    "- Reduce n: the whole idea is that we don't compare the new instance with all training examples; find ways to quickly identify m (<<n) potential near neighbors.\n",
    "### 4.2.1 K-D trees: low-dimensional, real-valued data\n",
    "- Step 1: pick random dimension;\n",
    "- Step 2: find median;\n",
    "- Step 3: split and repeat.\n",
    "<p align=\"center\">\n",
    "<img src=images/knn6.jpg width=\"300\" height=\"300\" alt=\"knn\" align=center>\n",
    "### 4.2.2 Inverted list examples: high-dimensional, discrete (sparse) data\n",
    "- Step 1: list all training examples that contain particular attribute;\n",
    "- Step 2: merge inverted lists for attributes present in new example;\n",
    "<p align=\"center\">\n",
    "<img src=images/knn8.jpg width=\"300\" height=\"200\" alt=\"knn\" align=center>\n",
    "### 4.2.3 Locality-Sensitive hashing: high-d, real valued or discrete\n",
    "- Step 1: slice the space into $2^k$ regions(polytopes);\n",
    "- Step 2: compare x only to training points in the same region R\n",
    "<p align=\"center\">\n",
    "<img src=images/KNN7.jpg width=\"300\" height=\"250\" alt=\"knn\" align=center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5. Practical issues\n",
    "- Equal number of positive/negative neighbours:\n",
    "  - use odd k to avoid the situation when we have equal number of positive/negative neighbours;\n",
    "  - random choose;\n",
    "  - prior: choose the class with greater prior;\n",
    "  - nearest: use 1-nn classifier.\n",
    "- Missing values:\n",
    "  - have to 'fill sth. in';\n",
    "  - reasonable choice: average value across entire dataset.\n",
    "## 6. Pros and cons\n",
    "- no assumptions about the data;\n",
    "- non-parametric approach;\n",
    "- need to handle missing data properly;\n",
    "- sensitive to class-outliers;\n",
    "- sensitive to lots of irrelevant attributes;\n",
    "- computationally expensive (space and time)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}