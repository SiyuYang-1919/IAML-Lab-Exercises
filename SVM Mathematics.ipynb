{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. SVMs overview:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 What does SVM learn from the training dataset and labeled data?\n",
    "- A linear model, or a line / hyperplane (for multiple variables);\n",
    "- Now, we have the equation that represents the 'line':\n",
    "$$ y = \\mathbf{w^{T}x} + w_0 $$\n",
    "- We use an algorithm to determine which are the values of W and b giving the 'best' line seperating the data;\n",
    "- SVM is one of the algorithms that help determine the two parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.2 Some background knowledge about SVM\n",
    "- SVMs include SVM (for classification) and SVR (for regression);\n",
    "- Four different SVM:\n",
    "  - The original one : the Maximal Margin Classifier,\n",
    "  - The kernelized version using the Kernel Trick,\n",
    "  - The soft-margin version,\n",
    "  - The soft-margin kernelized version (which combine 1, 2 and 3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.3 Comparison of SVMs and logistics regression\n",
    "- Logistics regression optimisation problem:\n",
    "$$ \\underset{\\theta}{\\text{min}} \\frac{1}{m} [\\sum_{i=1}^m y^{(i)}(-\\text{log}h_{\\theta}(x^{(i)}))+(1-y^{(i)})(-\\text{log}(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^n{\\theta_j}^2 $$\n",
    "- Support vector machine:\n",
    "$$ \\underset{\\theta}{\\text{min}} C\\sum_{i=1}^m [y^{(i)}\\text{cost}_1({\\theta}^T(x^{(i)})+(1-y^{(i)})(\\text{cost}_0{\\theta}^T(x^{(i)})]+\\frac{1}{2}\\sum_{j=1}^n{\\theta_j}^2 $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Understanding the Math of SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.1 The Margin (concept)\n",
    "1. ```The goal of SVM:```\n",
    "The goal of a support vector machine is to find  the optimal separating hyperplane which maximizes the margin of the training data. \n",
    "2. ```Optimal seperating hyperplane:```The fact that you can find a separating hyperplane,  does not mean it is the best one !\n",
    "<img src=https://www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1-separated-2.png width=\"300\" height=\"300\" alt=\"SVM\" align=center>\n",
    "\n",
    "So we will try to select an hyperplane as far as possible from data points from each category. The optimal seperating hyperplane should:\n",
    "- correctly classifies the training data;\n",
    "- generalize better with unseen data.\n",
    "3. ```Margin:``` the optimal hyperplane will be the one with the biggest margin."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.2 Margin Calculation\n",
    "<img src=images/Lab2/SVM1.jpg width=\"200\" height=\"200\" alt=\"SVM1\" align=center>\n",
    "<img src=images/Lab2/SVM2.jpg width=\"200\" height=\"200\" alt=\"SVM2\" align=center>\n",
    "\n",
    "- The distance of one training data (x) to the hyperplane is c, which is equal to |b-a|, while the margin is the distance from the closest training point to the hyperplane, minimize $c$;\n",
    "\n",
    "- Step 1: calculating b\n",
    "  - z (a vector) has the magnitude of b; its direction is the same as $w$, so its direction would be $\\frac{\\mathbf{w}}{||\\mathbf{w}||}$;\n",
    "  - That is, $z = b\\frac{\\mathbf{w}}{||\\mathbf{w}||}$;\n",
    "  - z on the hyperplane, so we have $ \\mathbf{w^Tz} + w_0 = 0$;\n",
    "  $$ \\mathbf{w^T} \\frac{b\\mathbf{w}}{||\\mathbf{w}||} + w_0 = 0 $$\n",
    "  $$ b||\\mathbf{w}|| + w_0 = 0 $$\n",
    "  $$ b = - \\frac{w_0}{||\\mathbf{w}||} $$\n",
    "  Note: $||\\mathbf{w}|| = \\sqrt{\\mathbf{w^Tw}}$\n",
    "- Step 2: calculating a\n",
    "  - $a$ is the magnituede of $x$'s projection on $w$;\n",
    "  - that is, \n",
    "  $$a = \\frac {\\mathbf{w^Tx}}{||\\mathbf{w}||}$$\n",
    "- Step 3: calculating c\n",
    "  - $ c = |b-a| = |\\frac{w_0}{||\\mathbf{w}||} + \\frac {\\mathbf{w^Tx}}{||\\mathbf{w}||}| $\n",
    "  - that is, the distance of one training point \n",
    "  $$ c = \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx} + w_0| $$\n",
    "- Step 4: the margin\n",
    "  - therefore, the margin is \n",
    "  $$ \\underset{i}{\\text{minimize}} \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx_i} + w_0| $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.3 The SVM optimisation problem\n",
    "- Scaling: $(\\mathbf{w}, w_0)$ and $(c\\mathbf{w}, cw_0)$ define the same hyperplane;\n",
    "- This is because: $c\\mathbf{w^Tx} + cw_0 \\geq 0$ is equal to $\\mathbf{w^Tx} + w_0 \\geq 0$;\n",
    "- Put a constraint on $(\\mathbf{w}, w_0)$, \n",
    "$$ \\underset{i}{\\text{min}} \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx_i} + w_0| = 1 $$\n",
    "- Now the margin will always be $\\frac{1}{||\\mathbf{w}||}$;\n",
    "- We want a hyperplane that will maximize the margin:\n",
    "$$ \\underset{\\mathbf{w}}{\\text{max}} \\frac{1}{||\\mathbf{w}||} $$\n",
    "subject to: \n",
    "$$\\mathbf{w^Tx_i} + w_0 \\geq 1 $$, for all i with $y_i = 1$; \n",
    "$$\\mathbf{w^Tx_i} + w_0 \\leq -1 $$, for all i with $y_i = -1$; \n",
    "$$ \\underset{i}{\\text{min}} \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx_i} + w_0| = 1 $$\n",
    "- After deleting the third rebundent restriction and simplifying the first two restrictions, we have:\n",
    "$$ \\underset{\\mathbf{w}}{\\text{max}} \\frac{1}{2||\\mathbf{w}||} $$\n",
    "subject to:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 $$, for all i\n",
    "- The above optimization is equal to:\n",
    "$$ \\underset{i}{\\text{min}} ||\\mathbf{w}||^2 $$\n",
    "subject to:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 $$, for all i"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.4 The solution of optimal paramters\n",
    "- ```Compute w:```\n",
    "$$ \\mathbf{w} = \\sum_i {\\alpha}_i{y_i}{x_i} $$\n",
    "- ```Compute w_0:```\n",
    "  - we can use a constraint to calculate $w_0$:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) = 1 $$\n",
    "  - multiply $y_i$ at each side (note ${y_i}^2 = 1$),\n",
    "$$ \\mathbf{w^Tx_i} + w_0 = y_i $$\n",
    "$$ w_0 = y_i - \\mathbf{w^Tx_i} $$\n",
    "- ```Hypothesis function:```\n",
    "  - therefore, prediction on new data point $x$ is:\n",
    "$$ f(x) = \\text{sign}((\\mathbf{w^Tx}) + w_0) $$\n",
    "$$ = \\text{sign}(\\sum_i^n {\\alpha}_i{y_i}({x_i^T}x) + w_0)$$\n",
    "- The formulation of the SVM is the hard margin SVM. It can not work when the data is not linearly separable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Soft Margin SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.1 When and how soft margin SVM helps?\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/Soft_SVM1.jpg width=\"200\" height=\"200\" alt=\"SVM3\" align=center>\n",
    "<img src=images/Lab2/Soft_SVM2.jpg width=\"200\" height=\"200\" alt=\"SVM4\" align=center>\n",
    "\n",
    "- Outlier reducing the margin vs. outlier breaking linear separability\n",
    "- Soft margin SVM allows mistakes, but should make as few mistakes as possible."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.2 $\\zeta$:\n",
    "- Therefore, we need to modify the constraints of the optimization problem, from...to...:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 $$, for all i\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i $$, for all i\n",
    "- However, if we choose a very large $\\zeta$, the constraint can be satisfied quite easily. To keep the mistakes as few as possible, we can modify the objective function to penalize the choice of $\\zeta$. That is,\n",
    "$$ \\underset{\\mathbf{w},b,\\zeta}{\\text{minimize}} \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^m \\zeta_i $$\n",
    "\n",
    "$$ \\text{subject to} \\quad y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i \\quad \\text{where} \\quad \\zeta_i \\geq 0 \\quad \\text{for any i=1,...,m} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.3 $C$:\n",
    "Generally speaking, parameter C will help us to determine how important the $\\zeta$ should be.\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/Soft_margin1.jpg width=\"500\" height=\"200\" alt=\"SVM5\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/Soft_margin2.jpg width=\"500\" height=\"200\" alt=\"SVM6\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/Soft_margin3.jpg width=\"500\" height=\"200\" alt=\"SVM7\" align=centering>\n",
    "\n",
    "  - 1) small C --> wider margin --> cost of some misclassifications;\n",
    "  - 2) big C --> hard margin --> no tolerance of misclassifications;\n",
    "  - 3) no magic value for C --> select C by grid search with cross-validation (note: C is specific to what we are using)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.4 2-Norm soft margin (L2 regularized)\n",
    "$$ \\underset{\\mathbf{w},b,\\zeta}{\\text{minimize}} \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^m \\zeta_i^2 $$\n",
    "\n",
    "$$ \\text{subject to} \\quad y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i \\quad \\text{where} \\quad \\zeta_i \\geq 0 \\quad \\text{for any i=1,...,m} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Kernals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.1 Feature mapping\n",
    "- In some case, the data is not linearly seperable, such as:\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/kernal1.jpg width=\"250\" height=\"200\" alt=\"SVM8\" align=centering>\n",
    "\n",
    "- Just as what we do in polynomial regression, we can do polynomial mapping to make: \n",
    "$$ \\phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3 $$\n",
    "defined by,\n",
    "$$ \\phi(x_1, x_2) = (x_1^2, \\sqrt{2}x_1x_2, x_2^2) $$\n",
    "- Now, the graph becomes:\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/kernal2.jpg width=\"250\" height=\"200\" alt=\"SVM8\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/kernal3.jpg width=\"250\" height=\"200\" alt=\"SVM8\" align=centering>\n",
    "\n",
    "- However, we have to try which transformation to apply dependent on the data that we have. [sklearn dataset transformation](https://scikit-learn.org/stable/data_transforms.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.2 What is and why we need a kernal?\n",
    "- Why: the feature mapping transformation will transform every example. It will take a huge amount of time to do it when we have millions of examples. Kernal does not need to transform every exmaple, we can compare the following two functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8100.0\n8100\n"
     ]
    }
   ],
   "source": [
    "# Transform a two-dimensional vector x into a three-dimensional vector\n",
    "import numpy as np \n",
    "def transform(x): \n",
    "    return [x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2]\n",
    "def polynomial_kernel(a, b): \n",
    "    return a[0]**2 * b[0]**2 + 2*a[0]*b[0]*a[1]*b[1] + a[1]**2 * b[1]**2\n",
    "\n",
    "x1 = [3,6] \n",
    "x2 = [10,10] \n",
    "x1_3d = transform(x1) \n",
    "x2_3d = transform(x2)\n",
    "print(np.dot(x1_3d,x2_3d))\n",
    "print(polynomial_kernel(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(a, b, degree, constant=0): \n",
    "    result = sum([a[i] * b[i] for i in range(len(a))]) + constant return pow(result, degree)\n",
    "# We do not transform the data\n",
    "print(polynomial_kernel(x1, x2, degree=2))"
   ]
  },
  {
   "source": [
    "As we can see, kernal allows us to deal with a large dataset.\n",
    "- What is a kernal?\n",
    "  - The kernal is defined by:\n",
    "  $$ K(\\mathbf{x_i}, \\mathbf{x_j}) = \\mathbf{x_i}^T·\\mathbf{x_j} $$\n",
    "  - A kernal is a function that returns the result of a dot product performed in another space.\n",
    "  - The polynomial_kernal computes their dot product as if they have been transformed into vectors belong to $\\mathbb{R^3}$\n",
    "- Kernal trick:\n",
    "  -  The original hypothesis function: \n",
    "$$ f(x) = \\text{sign}((\\mathbf{w^Tx}) + w_0) $$\n",
    "$$ = \\text{sign}(\\sum_i^n {\\alpha}_i{y_i}({x_i^T}x) + w_0)$$\n",
    "  - With kernal (note that SVM is a sparse kernal machines):\n",
    "$$ h(\\mathbf{x_i}) = \\text{sign}(\\sum_j^n {\\alpha}_j{y_j}K(\\mathbf{x_j}^T, \\mathbf{x_i}) + w_0)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.3 Kernal types\n",
    "- General form:\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\langle{\\phi(\\mathbf{x}^T), \\phi(\\mathbf{x'})}\\rangle_\\mathcal{V} $$\n",
    "- General rule:\n",
    "Try a RBF kernal first, because it uasually works well.\n",
    "- Linear Kernal\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\mathbf{x}^T·\\mathbf{x'} $$\n",
    "- Polynomial Kernal\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = (\\mathbf{x}^T·\\mathbf{x'} + w_0)^d $$\n",
    "(note: using a high-degree polynomial kernal will often lead to overfitting)\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/kernal4.jpg width=\"500\" height=\"200\" alt=\"SVM9\" align=centering>\n",
    "- RBF or Gaussian kernal\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\text{exp}(-\\gamma||\\mathbf{x} - \\mathbf{x'}||^2) $$\n",
    ", or\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\text{exp}(-\\frac{||\\mathbf{x} - \\mathbf{x'}||^2}{2{\\sigma}^2}) $$\n",
    "- the RBF(Radial Basis Function) returns the result of a dot product performed in $\\mathbb{R}^{\\infty} $\n",
    "- [How to choose gamma with sklearn](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/kernal6.jpg width=\"300\" height=\"200\" alt=\"SVM11\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/Lab2/kernal5.jpg width=\"500\" height=\"200\" alt=\"SVM10\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## References:\n",
    "- [markdown image](https://stackoverflow.com/questions/12090472/github-readme-md-center-image)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}